{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_3HouNb9uxK"
      },
      "source": [
        "# Scrape GitHub for top repositories"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kh0JF92l_8d5"
      },
      "source": [
        "Web scraping is the process of using bots to extract content and data from a website.\n",
        "Web Scraping helps in doing market research and allows business firms to keep track of other compititor firms in the market."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ETIak3cdCpsK"
      },
      "source": [
        "In this project I have made a simple python program using BeautifulSoup, which scrapes the topics page on GitHub to collect the top repositories for each of the top topics.\n",
        "\n",
        "https://github.com/topics/\n",
        "\n",
        "I am using Pyhton due to its amazing libraries and personal preference. I am using the BeautifulSoup library for this project because the GitHub page is not dynamic."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4lllcadDFac"
      },
      "source": [
        "General outline of the steps that we will follow are : \n",
        "\n",
        "*   Scrape the main topics page from GitHub\n",
        "*   Extract the topic title, topic description and url from each of the topics.\n",
        "*   Then for each of the individual topics, we will scrape again to get the top repositories on that topic.\n",
        "*   We will collect the name of the repository, author the reposotory, and url of the repository and store in a csv file.\n",
        "*   Finally we will store all of these data in a folder.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDcRxjckI1U1"
      },
      "source": [
        "First of all let's install and import all the necessary libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "I4W24Pu_I-ag"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: You are using pip version 21.2.3; however, version 21.3.1 is available.\n",
            "You should consider upgrading via the 'c:\\program files\\python39\\python.exe -m pip install --upgrade pip' command.\n",
            "WARNING: You are using pip version 21.2.3; however, version 21.3.1 is available.\n",
            "You should consider upgrading via the 'c:\\program files\\python39\\python.exe -m pip install --upgrade pip' command.\n",
            "WARNING: You are using pip version 21.2.3; however, version 21.3.1 is available.\n",
            "You should consider upgrading via the 'c:\\program files\\python39\\python.exe -m pip install --upgrade pip' command.\n"
          ]
        }
      ],
      "source": [
        "!pip install requests --quiet\n",
        "!pip install beautifulsoup4 --quiet\n",
        "!pip install pandas --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "NmZpm2owJDJ_"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rRwcJyntIb2p"
      },
      "source": [
        "## Scraping the topics page from GitHub."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "eQkIzIiJIZv0"
      },
      "outputs": [],
      "source": [
        "def get_main_page():\n",
        "  # Function to download and process the main page.\n",
        "\n",
        "  # First we will use requests to download the page.\n",
        "  # PS.: If we wish to scrape the second page of the topics then we have to add\n",
        "  #  \"/page?=2\" at the end of the given url.\n",
        "  topic_url = 'https://github.com/topics'\n",
        "  response = requests.get(topic_url)\n",
        "\n",
        "  # Check if the download was successful.\n",
        "  if response.status_code != 200:\n",
        "    raise Exception('Failed to load page {}'.format(topic_url))\n",
        "  \n",
        "  # Since this page is not dynamic we can use BeautifulSoup to parse it.\n",
        "  doc = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "  # Return the processed page.\n",
        "  return doc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zN84wevBMxlY"
      },
      "source": [
        "## Extracting the topics information."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "navpp6dEM50D"
      },
      "source": [
        "Here we have 3 functions to extract 3 details, ie., name of the topic, description of the topic, and url of the topic page."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kMgifSQ3NP_T"
      },
      "source": [
        "In each of fuctions we take the processed page and search for the tags which contain the required information for each topic. We do this using the BeautifulSoup fuction find_all(). It searches the page and return the tags we want. We can specify a particular class along with it to narrow down our search.\n",
        "\n",
        "We can use the inspect element feature to get the particular class and type of tag for the data we want."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "UAcc-IT_M3zi"
      },
      "outputs": [],
      "source": [
        "def get_topic_titles(doc):\n",
        "  # Function to extract the titles of the topics.\n",
        "\n",
        "  # The tag which contains the title of the topics is a 'p' tag and has the \n",
        "  # following class.\n",
        "  topic_title_tags_class = 'f3 lh-condensed mb-0 mt-1 Link--primary'\n",
        "  topic_title_tags = doc.find_all('p', {'class': topic_title_tags_class})\n",
        "\n",
        "  # We store all the titles in a list.\n",
        "  topic_titles = []\n",
        "\n",
        "  for tag in topic_title_tags:\n",
        "    topic_titles.append(tag.text)\n",
        "\n",
        "  return topic_titles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "4rwFgpFROlhR"
      },
      "outputs": [],
      "source": [
        "def get_topic_decs(doc):\n",
        "  # Function to extract the descriptions of the topics.\n",
        "\n",
        "  # The tag which contains the description is also a 'p' tag has the following \n",
        "  # class.\n",
        "  topic_desc_tag_class = 'f5 color-text-secondary mb-0 mt-1'\n",
        "  topic_desc_tag = doc.find_all('p', {'class': topic_desc_tag_class})\n",
        "\n",
        "  # We store it in a list.\n",
        "  topic_desc = []\n",
        "\n",
        "  for tag in topic_desc_tag:\n",
        "    topic_desc.append(tag.text.strip())\n",
        "\n",
        "  return topic_desc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "2dCs21JDSUxp"
      },
      "outputs": [],
      "source": [
        "def get_topic_urls(doc):\n",
        "  # Function to extract the urls of the topics.\n",
        "\n",
        "  # The tag which contains the url is a 'a' tag having the following class.\n",
        "  topic_link_tag_class = 'd-flex no-underline'\n",
        "  topic_link_tag = doc.find_all('a', {'class': topic_link_tag_class})\n",
        "\n",
        "  # We store the urls along with the base url to make it a fully functioning \n",
        "  # url.\n",
        "  topic_urls = []\n",
        "  base_url = 'https://github.com'\n",
        "\n",
        "  for tag in topic_link_tag:\n",
        "    topic_urls.append(base_url + tag['href'])\n",
        "\n",
        "  return topic_urls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNSCJoEqTAPS"
      },
      "source": [
        "Now let's have one function which calls the above fuctions to complete the first part."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "VdO913MYTYdB"
      },
      "outputs": [],
      "source": [
        "def scrape_topics():\n",
        "  # Function to scrape the topics page and save it in a dataframe.\n",
        "\n",
        "  # Gets the main topics page.\n",
        "  doc = get_main_page()\n",
        "\n",
        "  # Creats a dictionary to store all the details.\n",
        "  topics_dict = {\n",
        "      'title': get_topic_titles(doc),\n",
        "      'description': get_topic_decs(doc),\n",
        "      'url': get_topic_urls(doc)\n",
        "  }\n",
        "\n",
        "  # Makes the dictionary into a dataframe and return it.\n",
        "  return pd.DataFrame(topics_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4v2MmfxIZgU"
      },
      "source": [
        "## Scrape each individual topics for the repositories list."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JabhhCZoIiyd"
      },
      "source": [
        "Now we use the dataframe created previously and use the link given for each topic to download that page and get the top repositories."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "OU8yzKyOIhBM"
      },
      "outputs": [],
      "source": [
        "def download_a_repo(link_to_repo):\n",
        "  # Function to download and process the repository page.\n",
        "\n",
        "  # First of all we download the repository page using the url from the \n",
        "  # dataframe.\n",
        "  response = requests.get(link_to_repo)\n",
        "\n",
        "  # Check if the download was successful.\n",
        "  if response.status_code != 200:\n",
        "    raise Exception('Failed to load the page {}'.format(link_to_repo))\n",
        "  \n",
        "  # Process the page using BeautifulSoup and return it.\n",
        "  page_doc = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "  return page_doc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vn1XlWOIWZdB"
      },
      "source": [
        "## Extract the details of the top repositories for a topic."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57eoPomlJ2fk"
      },
      "source": [
        "To make things simpler let's make a helper function that will help us to extract infromation from the repository page. This helper function will take one of the repositories in a topic and extract its details. We would also require a function to convert strings like \"52k\" into integer 52000, as the number of stars is given inthis format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "-kBZLdIqaYEc"
      },
      "outputs": [],
      "source": [
        "def parse_stringToInt(count):\n",
        "  # Function to convert a string such as \"52k\" to integer 52000.\n",
        "\n",
        "  # First we remove the excess spaces.\n",
        "  count = count.strip()\n",
        "\n",
        "  # If there is a \"k\" then we remove the \"k\" and multiply by 1000.\n",
        "  if count[-1] == 'k':\n",
        "    return int(float(count[:-1]) * 1000)\n",
        "  \n",
        "  return int(count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "oKD6sDmjKJGV"
      },
      "outputs": [],
      "source": [
        "def get_repo_info_helper(parent_tag, star_tag):\n",
        "  # Function which returns all the details of a repository.\n",
        "\n",
        "  # In the website both the name of the project and name of the author are kept\n",
        "  #  under one 'a' tag. We can extract it and the get both its child to get \n",
        "  # what we want.\n",
        "  child_tags = parent_tag.find_all('a')\n",
        "\n",
        "  author = child_tags[0].text.strip()\n",
        "  name = child_tags[1].text.strip()\n",
        "\n",
        "  # In the website the url of the repository is conveniently kept in the repo \n",
        "  # name tag itself. So we take it and add the base url.\n",
        "  base_url = 'https://github.com'\n",
        "  url = base_url + child_tags[1]['href']\n",
        "\n",
        "  # For the number of stars we take the star_tag and extract and process the \n",
        "  # text using the above function.\n",
        "  stars = parse_stringToInt(star_tag.text.strip())\n",
        "\n",
        "  # We return all 4 details in order.\n",
        "  return author, name, stars, url"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "556s1ariRIJu"
      },
      "source": [
        "The following function will use the above helper function and extract details for all of the top repositories in a topic."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Sie_JTewRPNF"
      },
      "outputs": [],
      "source": [
        "def get_repo_info(page_doc):\n",
        "  # Function which extracts the details of all the top repositories of a topic.\n",
        "\n",
        "  # We use the inspect feature to get the tag type and class of the required \n",
        "  # tags.\n",
        "  repo_tags_class = 'f3 color-fg-muted text-normal lh-condensed'\n",
        "  repo_tags = page_doc.find_all('h3', {'class': repo_tags_class})\n",
        "\n",
        "  repo_star_class = 'social-count float-none'\n",
        "  repo_star = page_doc.find_all('a', {'class': repo_star_class})\n",
        "\n",
        "  # We create a dictionary to store all the results.\n",
        "  repo_dict = {\n",
        "      'username': [],\n",
        "      'repo_name': [],\n",
        "      'stars': [],\n",
        "      'url': []\n",
        "  }\n",
        "\n",
        "  # We fill the dictionary by repeatedly calling the helper function for all \n",
        "  # the repositories in a topic.\n",
        "  for i in range(len(repo_tags)):\n",
        "    repo_info = get_repo_info_helper(repo_tags[i], repo_star[i])\n",
        "    repo_dict['username'].append(repo_info[0])\n",
        "    repo_dict['repo_name'].append(repo_info[1])\n",
        "    repo_dict['stars'].append(repo_info[2])\n",
        "    repo_dict['url'].append(repo_info[3])\n",
        "\n",
        "  # We convert this into a dataframe and return it.\n",
        "  return pd.DataFrame(repo_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "byysqCzLUJDE"
      },
      "source": [
        "Now we have the data we require, all that is left is to make a csv file and save it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "JAncBM83UTOD"
      },
      "outputs": [],
      "source": [
        "def csv_repo(url, path):\n",
        "  # Function which takes a topic url and extracts the details of all the \n",
        "  # repositories in it and saves it in a csv file in the given path.\n",
        "\n",
        "  # Checks if the path provided already exists or not.\n",
        "  if os.path.exists(path):\n",
        "    print('The file {} already exists. Skipping...'.format(path))\n",
        "    return\n",
        "\n",
        "  # We call the above fuctions one by one and save the result in a csv file.\n",
        "  repo_df = get_repo_info(download_a_repo(url))\n",
        "  repo_df.to_csv(path, index=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2FBUfLCPXBCh"
      },
      "source": [
        "## Putting everything together."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oGLY0pnTYJik"
      },
      "source": [
        "Here we make the master function which uses all the  above functions and creates a folder to store the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "LLEuz6R6YV_c"
      },
      "outputs": [],
      "source": [
        "def scrape_github_topics():\n",
        "  # Function to scrape the topics page of GitHub and store the details of the \n",
        "  # top repositories of the top topics.\n",
        "\n",
        "  # First we scrape the main page and get the topics list.\n",
        "  print('Scraping list of topics :')\n",
        "  topics_df = scrape_topics()\n",
        "\n",
        "  # Let's save the list of the topics as a csv file as well.\n",
        "  topics_df.to_csv('topics.csv', index=None)\n",
        "\n",
        "  # Make a folder to store the data neatly.\n",
        "  os.makedirs('data', exist_ok=True)\n",
        "\n",
        "  # We go through the topics list and scrape each topic to extract the details \n",
        "  # of the top repositories in it and make a csv file and store it in the \n",
        "  # folder.\n",
        "  for index, row in topics_df.iterrows():\n",
        "    print('Scraping top repositories for \"{}\"'.format(row['title']))\n",
        "    csv_repo(row['url'], 'data/{}.csv'.format(row['title']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gtr3H9uDZiZJ"
      },
      "source": [
        "Now let's run the entire thing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s9e36zQ5Znfe",
        "outputId": "1e74b4f7-e5f7-4303-8b03-1d0c9170a42d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Scraping list of topics :\n",
            "Scraping top repositories for \"3D\"\n",
            "The file data/3D.csv already exists. Skipping...\n",
            "Scraping top repositories for \"Ajax\"\n",
            "The file data/Ajax.csv already exists. Skipping...\n",
            "Scraping top repositories for \"Algorithm\"\n",
            "The file data/Algorithm.csv already exists. Skipping...\n",
            "Scraping top repositories for \"Amp\"\n",
            "The file data/Amp.csv already exists. Skipping...\n",
            "Scraping top repositories for \"Android\"\n",
            "The file data/Android.csv already exists. Skipping...\n",
            "Scraping top repositories for \"Angular\"\n",
            "The file data/Angular.csv already exists. Skipping...\n",
            "Scraping top repositories for \"Ansible\"\n",
            "The file data/Ansible.csv already exists. Skipping...\n",
            "Scraping top repositories for \"API\"\n",
            "The file data/API.csv already exists. Skipping...\n",
            "Scraping top repositories for \"Arduino\"\n",
            "The file data/Arduino.csv already exists. Skipping...\n",
            "Scraping top repositories for \"ASP.NET\"\n",
            "The file data/ASP.NET.csv already exists. Skipping...\n",
            "Scraping top repositories for \"Atom\"\n",
            "The file data/Atom.csv already exists. Skipping...\n",
            "Scraping top repositories for \"Awesome Lists\"\n",
            "The file data/Awesome Lists.csv already exists. Skipping...\n",
            "Scraping top repositories for \"Amazon Web Services\"\n",
            "The file data/Amazon Web Services.csv already exists. Skipping...\n",
            "Scraping top repositories for \"Azure\"\n",
            "The file data/Azure.csv already exists. Skipping...\n",
            "Scraping top repositories for \"Babel\"\n",
            "The file data/Babel.csv already exists. Skipping...\n",
            "Scraping top repositories for \"Bash\"\n",
            "The file data/Bash.csv already exists. Skipping...\n",
            "Scraping top repositories for \"Bitcoin\"\n",
            "The file data/Bitcoin.csv already exists. Skipping...\n",
            "Scraping top repositories for \"Bootstrap\"\n",
            "The file data/Bootstrap.csv already exists. Skipping...\n",
            "Scraping top repositories for \"Bot\"\n",
            "The file data/Bot.csv already exists. Skipping...\n",
            "Scraping top repositories for \"C\"\n",
            "The file data/C.csv already exists. Skipping...\n",
            "Scraping top repositories for \"Chrome\"\n",
            "The file data/Chrome.csv already exists. Skipping...\n",
            "Scraping top repositories for \"Chrome extension\"\n",
            "The file data/Chrome extension.csv already exists. Skipping...\n",
            "Scraping top repositories for \"Command line interface\"\n",
            "The file data/Command line interface.csv already exists. Skipping...\n",
            "Scraping top repositories for \"Clojure\"\n",
            "Scraping top repositories for \"Code quality\"\n",
            "Scraping top repositories for \"Code review\"\n",
            "Scraping top repositories for \"Compiler\"\n",
            "Scraping top repositories for \"Continuous integration\"\n",
            "Scraping top repositories for \"COVID-19\"\n",
            "Scraping top repositories for \"C++\"\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    scrape_github_topics()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wKNTXTlndxgk"
      },
      "source": [
        "## Summary and Scope for future works."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FE70gASQd3y-"
      },
      "source": [
        "Summary:\n",
        "\n",
        "In this project I have tried to implement a simple web scraping program, which is used to scrape the details of the top repositories of the top topics on the site.\n",
        "\n",
        "I have used the Python language and the BeautifulSoup library in it for the project.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hVC_efgAiE8N"
      },
      "source": [
        "Ideas for future work:\n",
        "\n",
        "This project can serve as a foundations for various different applications such as:\n",
        "\n",
        "\n",
        "*   Analysing how useful a particular repository is.\n",
        "*   Analysing how active a particular topic is.\n",
        "*   Keeping a track of the latest developments in an area.\n",
        "*   and many others......"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4Wb16PPiOF4"
      },
      "source": [
        "Link for the BeautifulSoup library :\n",
        "\n",
        "https://www.crummy.com/software/BeautifulSoup/bs4/doc/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPetzlO7i5t8"
      },
      "source": [
        "PS.: We can save the data in a cloud storage directly by first mounting the drive and then copying it there."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "GitHubWebScraping.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "ac59ebe37160ed0dfa835113d9b8498d9f09ceb179beaac4002f036b9467c963"
    },
    "kernelspec": {
      "display_name": "Python 3.9.6 64-bit",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
